{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a856803-9487-4adb-bff2-c9e0e14ba759",
   "metadata": {},
   "source": [
    "# Teradata as Embeddings Storage. Semantic search\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Embeddings are revolutionizing the way we process and understand text data. These dense vector representations capture the semantic essence of words, phrases, and even entire documents, enabling machines to grasp nuanced meanings and relationships within the data. Imagine a high-dimensional space where each point represents a word, phrase, or document as a vector. The position of each point is determined by the context in which it appears, meaning similar concepts are located closer together.\n",
    "\n",
    "For instance, the words \"king\" and \"queen\" would have vectors that are close to each other, as would \"Paris\" and \"France.\" This spatial proximity allows embeddings to capture intricate relationships and patterns that traditional keyword-based methods miss.\n",
    "\n",
    "Unlike traditional keyword-based search methods, embedding-based search leverages these rich representations to find relevant information based on context and meaning rather than exact word matches. This is where cosine similarity comes into play. By measuring the cosine of the angle between vectors, cosine similarity allows us to quantify how similar two embeddings are, effectively identifying the most relevant documents or texts. This technique is crucial in applications ranging from information retrieval and recommendation systems to natural language understanding, providing more accurate and meaningful search results.\n",
    "\n",
    "### Illustration of Embeddings\n",
    "\n",
    "To visualize embeddings, imagine a 2D plot (though embeddings typically exist in much higher dimensions):\n",
    "\n",
    "![alt text](img/embeddings_1.jpg \"Embeddings: King and Queen, Paris and France\")\n",
    "\n",
    "\n",
    "In this illustration:\n",
    "- \"King\" and \"Queen\" are close together, indicating they are semantically similar.\n",
    "- \"Paris\" and \"France\" are also close together, showing a geographical relationship.\n",
    "\n",
    "By using embeddings, we can better understand and search through our data in ways that are meaningful and contextually relevant.\n",
    "\n",
    "## Approach\n",
    "\n",
    "In this demo, we showcase an advanced approach to embedding-based search using the Teradata database. Our methodology involves several key steps:\n",
    "\n",
    "1. **Importing and Converting Model**: We begin by importing pre-trained models from Hugging Face, which are renowned for their ability to capture semantic meanings in text data effectively. To enhance performance and ensure compatibility with various execution environments, we convert these Hugging Face models into the ONNX (Open Neural Network Exchange) format using the [`optimum`](https://github.com/huggingface/optimum) utility.\n",
    "\n",
    "2. **Model Deployment to Database to be Used with BYOM**: Leveraging Teradata's BYOM (Bring Your Own Model) capability, we deploy the model directly within the Teradata database. This integration minimizes data movement and optimizes performance by keeping the model execution close to the data storage.\n",
    "\n",
    "3. **In-Database Embedding Generation and Building the Embedding Store**: We execute the embedding generation process directly within the Teradata database. Each text entry in our knowledge base is processed to create its corresponding embedding vector, which is then stored in a structured repository for efficient retrieval.\n",
    "\n",
    "4. **Semantic Search with Cosine Similarity**: Finally, we utilize Teradata’s functionality to calculate cosine similarity between a query embedding and the embeddings stored in the database. Cosine similarity, which measures the angle between two vectors, effectively determines their similarity. This enables us to perform semantic searches directly within the database, retrieving the most relevant results based on the meaning of the text rather than exact keyword matches.\n",
    "\n",
    "The advantage of this approach is that the data never leaves the database. This ensures data security and compliance while reducing latency and improving efficiency, as all operations are performed close to where the data resides.\n",
    "\n",
    "This approach combines state-of-the-art embedding models with Teradata's robust data management and processing capabilities, facilitating efficient and accurate semantic searches at scale.\n",
    "\n",
    "\n",
    "![alt text](img/embeddings_diagram.jpg \"Teradata in-database Embedding Store\")\n",
    "\n",
    "The advantage of this approach is that the data never leaves the database. This ensures data security and compliance while reducing latency and improving efficiency, as all operations are performed close to where the data resides.\n",
    "\n",
    "This approach combines state-of-the-art embedding models with Teradata's robust data management and processing capabilities, facilitating efficient and accurate semantic searches at scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24925fa-132f-4d3b-aa26-5980f04e5cfc",
   "metadata": {},
   "source": [
    "## Part 1. Importing and Converting Model\n",
    "\n",
    "We start by importing the pre-trained [BAAI/bge](https://huggingface.co/BAAI/bge-small-en-v1.5) model from Hugging Face, renowned for its effectiveness in capturing semantic meanings in text data. The BAAI/bge model is a state-of-the-art model trained on a large corpus, capable of generating high-quality text embeddings.\n",
    "\n",
    "To enhance performance and ensure compatibility with various execution environments, we'll use the Optimum utility to convert the model into the ONNX (Open Neural Network Exchange) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b0ed1-f1ec-4b89-9722-8ddd95fb021b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! optimum-cli export onnx --opset 16 --trust-remote-code -m sonoisa/sentence-bert-base-ja-mean-tokens-v2 sentence-bert-base-ja-mean-tokens-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1e619-83e9-41c5-985b-76de6c167bed",
   "metadata": {},
   "source": [
    "After conversion to ONNX, we perform the following fixes:\n",
    "- Fixing dynamic dimensions on input and output, ensuring compatibility with different input sizes.\n",
    "- Fixing the opset in the ONNX file for compatibility with ONNX runtime.\n",
    "- Removing tokens embeddings output to save I/O during processing, optimizing the model for efficient execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867938e-66b9-4a3f-9dfa-87242c3ae915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as rt\n",
    "\n",
    "import transformers\n",
    "from onnxruntime.tools.onnx_model_utils import *\n",
    "\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import teradataml as tdml\n",
    "\n",
    "op = onnx.OperatorSetIdProto()\n",
    "\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "import getpass\n",
    "\n",
    "op.version = 16\n",
    "\n",
    "model = onnx.load('sentence-bert-base-ja-mean-tokens-v2/model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66422f26-6ba0-459a-af7c-7a1130f5bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_ir8 = onnx.helper.make_model(model.graph, ir_version = 8, opset_imports = [op]) #to be sure that we have compatible opset and IR version\n",
    "\n",
    "\n",
    "# fixing the variable dim sizes in our mode\n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"batch_size\", 1) \n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"sequence_length\", 512)\n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"Concatsentence_embedding_dim_1\", 768)\n",
    "\n",
    "\n",
    "#remove useless token_embeddings output from the model\n",
    "for node in model_ir8.graph.output:\n",
    "    if node.name == \"token_embeddings\":\n",
    "        model_ir8.graph.output.remove(node)\n",
    "\n",
    "#saving the model\n",
    "onnx.save(model_ir8, 'sentence-bert-base-ja-mean-tokens-v2/model_fixed.onnx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed8c4a-264b-4e4f-abf7-98107bfd5a51",
   "metadata": {},
   "source": [
    "Once the fixes are applied, we proceed to test the correctness of the ONNX model by calculating cosine similarity between two texts using native SentenceTransformers and ONNX runtime, comparing the results.\n",
    "\n",
    "If the results are identical, it confirms that the ONNX model gives the same result as the native models, validating its correctness and suitability for further use in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe79715-e95f-4cfe-ae8b-3fafc806514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1 = '今日の天気はいかがですか？'\n",
    "sentences_2 = '今日の天気はどうですか？'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03b7ea-9ceb-4d49-b0da-cbeca116d82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ONNX result\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"./sentence-bert-base-ja-mean-tokens-v2\")\n",
    "predef_sess = rt.InferenceSession(\"sentence-bert-base-ja-mean-tokens-v2/model_fixed.onnx\")\n",
    "\n",
    "enc1 = tokenizer(sentences_1, max_length = 512, padding='max_length' )\n",
    "embeddings_1_onnx = predef_sess.run(None,     {\"input_ids\": [enc1.input_ids], \n",
    "     \"attention_mask\": [enc1.attention_mask]})\n",
    "\n",
    "enc2 = tokenizer(sentences_2, max_length = 512, padding='max_length' )\n",
    "embeddings_2_onnx = predef_sess.run(None,     {\"input_ids\": [enc2.input_ids], \n",
    "     \"attention_mask\": [enc2.attention_mask]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09858719-7480-4862-abbb-6a35de381da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate native model result using SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sonoisa/sentence-bert-base-ja-mean-tokens-v2')\n",
    "embeddings_1_sentence_transformer = model.encode(sentences_1, normalize_embeddings=True)\n",
    "embeddings_2_sentence_transformer = model.encode(sentences_2, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e9a52-4e8c-44c6-8321-601a43189abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "\n",
    "print(\"Cosine similiarity for embeddings calculated with ONNX:\" + str(cos_sim(embeddings_1_onnx[0][0], embeddings_2_onnx[0][0])))\n",
    "print(\"Cosine similiarity for embeddings calculated with SentenceTransformer:\" + str(cos_sim(embeddings_1_sentence_transformer, embeddings_2_sentence_transformer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6d5b3-3c15-4970-9a0b-8ef67d907eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizerFast.from_pretrained(\"sentence-bert-base-ja-mean-tokens-v2\", \n",
    "                                      local_files_only=True, \n",
    "                                      use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da8a55-5fea-4849-886c-c9b6c2e23cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"sentence-bert-base-ja-mean-tokens-v2-tokenizer\", legacy_format=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df79517-1e4f-420a-b7e3-fbb2bb44009d",
   "metadata": {},
   "source": [
    "## Part 2. Model Deployment to Database to be Used with BYOM\n",
    "\n",
    "In this section, we demonstrate how to deploy the model to the Teradata database using the BYOM (Bring Your Own Model) capability. We use the `teradataml` Python library to manage the connectivity and provide a convenient Python API that is similar to PySpark or pandas DataFrame.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b98f37-9248-4dd4-9313-867fc3036156",
   "metadata": {},
   "source": [
    "### Opening Connection to Teradata\n",
    "\n",
    "We start by setting up a connection to the Teradata database. The `teradataml` library handles all the intricacies of database connectivity, allowing us to interact with Teradata in a manner similar to working with data in pandas DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e801c-31af-48bc-af16-0f150cec7c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tdml.create_context(host = 'teradata', username='<YOUR DATABASE USER>', password = getpass.getpass('YOUR DATABASE PASSWORD'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9d126-a4f2-4cd2-b170-78927c93dfb1",
   "metadata": {},
   "source": [
    "### Deploying the Model and Tokenizer\n",
    "\n",
    "After establishing the connection, we deploy two key artifacts to the database:\n",
    "1. The model itself, converted to ONNX format.\n",
    "2. The `tokenizer.json` file, which will be used for in-database tokenization.\n",
    "\n",
    "Both artifacts are deployed using the `save_byom` function, which abstracts the underlying complexity and makes the deployment process straightforward. Internally, this function performs an insert operation into the database.\n",
    "\n",
    "By using the `save_byom` function, we ensure that our model and tokenizer are readily available within the Teradata database for subsequent embedding generation and semantic search operations. This integration minimizes data movement and optimizes performance by keeping all operations within the database environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91ebb6-22a3-4d6e-b584-a266cdb371a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT IF TABLE EXISTS\n",
    "#tdml.db_drop_table('embeddings_models_jp')\n",
    "tdml.save_byom('sentence-bert-base-ja-mean-tokens-v2',\n",
    "              'sentence-bert-base-ja-mean-tokens-v2/model_fixed.onnx',\n",
    "              'embeddings_models_jp')\n",
    "\n",
    "#UNCOMMENT IF TABLE EXISTS\n",
    "#tdml.db_drop_table('embeddings_tokenizers_jp')\n",
    "tdml.save_byom('sentence-bert-base-ja-mean-tokens-v2',\n",
    "              'sentence-bert-base-ja-mean-tokens-v2-tokenizer/tokenizer.json',\n",
    "              'embeddings_tokenizers_jp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d755479-839b-4c36-b6f9-6d3026bd9abb",
   "metadata": {},
   "source": [
    "## Part 3. In-Database Embedding Generation and Building the Embedding Store\n",
    "\n",
    "In this point, we are taking the history of the emails and building the embedding store in three simple steps:\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Using the `tokenizer_encode` function and the `tokenizer.json` file we deployed in the previous step, we perform tokenization of the input texts. Tokenization is the process of converting a sequence of text into individual tokens, which are the building blocks that the model uses to understand and process the text. For example, the sentence \"Hello, how are you?\" would be tokenized into the following tokens: [\"Hello\", \",\", \"how\", \"are\", \"you\", \"?\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fbd274-4349-4fcb-aad0-0d068213cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.DataFrame(tdml.in_schema('emails', 'emails_jp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11cdbe4-e229-4394-9f78-592f2eba50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.execute_sql(\"\"\"\n",
    "\n",
    "replace view v_emails_tokenized_for_embeddings_jp as (\n",
    "    select\n",
    "        id,\n",
    "        txt,\n",
    "        txt_jp,\n",
    "        IDS as input_ids,\n",
    "        attention_mask\n",
    "    from ivsm.tokenizer_encode(\n",
    "        on (select top 100 * from emails.emails_jp)\n",
    "        on (select model as tokenizer from embeddings_tokenizers_jp where model_id = 'sentence-bert-base-ja-mean-tokens-v2') DIMENSION\n",
    "        USING\n",
    "            ColumnsToPreserve('id', 'txt', 'txt_jp')\n",
    "            OutputFields('IDS', 'ATTENTION_MASK')\n",
    "            MaxLength(512)\n",
    "            PadToMaxLength('True')\n",
    "            TokenDataType('INT64')\n",
    "    ) a\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb5334-a4fb-405c-b759-0c644d7db2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.DataFrame('v_emails_tokenized_for_embeddings_jp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5cafc-c1f8-45c8-8f19-51384411f4e9",
   "metadata": {},
   "source": [
    "### Embedding Generation\n",
    "\n",
    "At this stage, we generate embeddings from the tokenized texts using the model previously deployed. We use the `iVSM_score` function, which outputs embeddings in binary representation as a field in a table of BLOB datatype. Embeddings are dense vector representations of text that capture the semantic meaning of the text in a multi-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54badc-3660-42fc-9c8f-ca746473a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.execute_sql(\"\"\"\n",
    "\n",
    "replace view emails_embeddings_jp as (\n",
    "    select \n",
    "            *\n",
    "    from ivsm.IVSM_score(\n",
    "            on v_emails_tokenized_for_embeddings_jp  -- table with data to be scored\n",
    "            on (select * from embeddings_models_jp where model_id = 'sentence-bert-base-ja-mean-tokens-v2') dimension\n",
    "            using\n",
    "                ColumnsToPreserve('id', 'txt', 'txt_jp') -- columns to be copied from input table\n",
    "                ModelType('ONNX') -- model format\n",
    "                BinaryInputFields('input_ids', 'attention_mask') -- enables binary input vectors\n",
    "                BinaryOutputFields('sentence_embedding')\n",
    "                Caching('inquery') -- tun on model caching within the query\n",
    "        ) a \n",
    ")\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de642335-d9a6-4c8c-8134-106a70d329a4",
   "metadata": {},
   "source": [
    "### Exploding Binary Representation\n",
    "\n",
    "Finally, we explode the binary representation of the embeddings into 384 columns of FLOAT datatype. When embeddings are represented this way, they can be consumed by the `TD_VectorDistance` function in Teradata to perform semantic search using cosine similarity. Cosine similarity measures the angle between two vectors, effectively determining their similarity based on their direction in the multi-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bfa2f9-ecef-4991-82d2-05977ae11e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT IF TABLE EXISTS\n",
    "tdml.db_drop_table('emails_embeddings_store_jp')\n",
    "\n",
    "tdml.execute_sql(\"\"\"\n",
    "\n",
    "create table emails_embeddings_store_jp as (\n",
    "    select \n",
    "            *\n",
    "    from ivsm.vector_to_columns(\n",
    "            on emails_embeddings_jp\n",
    "            using\n",
    "                ColumnsToPreserve('id', 'txt', 'txt_jp') \n",
    "                VectorDataType('FLOAT32')\n",
    "                VectorLength(768)\n",
    "                OutputColumnPrefix('emb_')\n",
    "                InputColumnName('sentence_embedding')\n",
    "        ) a \n",
    ") with data\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4d135-3988-4c55-8d69-6ff2e04fb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_embeddings_store = tdml.DataFrame('emails_embeddings_store_jp')\n",
    "tdf_embeddings_store.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773523c-f6d1-4eaa-b8a6-a73c64e2cce7",
   "metadata": {},
   "source": [
    "By following these steps, we efficiently generate and store embeddings within the Teradata database, making them readily available for high-performance semantic search operations.\n",
    "\n",
    "\n",
    "Building the embedding store directly within the Teradata database is both important and beneficial for several reasons:\n",
    "\n",
    "- **Performance**: By generating and storing embeddings in-database, we reduce data movement and leverage Teradata’s powerful processing capabilities. This results in faster query execution and lower latency.\n",
    "\n",
    "- **Scalability**: Teradata is designed to handle large-scale data. Embedding generation and storage within Teradata ensures that we can scale our operations to handle vast amounts of text data without compromising on performance.\n",
    "\n",
    "- **Security**: Keeping data within the database ensures that sensitive information remains secure and complies with data governance policies. There is no need to move data to external systems for processing.\n",
    "\n",
    "- **Integration**: Embedding the store directly in Teradata allows seamless integration with existing data and applications. This enables more comprehensive data analysis and supports advanced use cases such as real-time semantic search and analytics.\n",
    "\n",
    "By leveraging Teradata's robust infrastructure and advanced capabilities, we can build an efficient, secure, and scalable embedding store that enhances our ability to perform sophisticated text analysis and semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896e98e-2057-4f81-98bb-00941f37ca81",
   "metadata": {},
   "source": [
    "## Part 4. Semantic Search with Cosine Similarity\n",
    "\n",
    "In this final step, we perform semantic search using cosine similarity within the Teradata database. We utilize the `TD_VectorDistance` function, which is specifically designed for calculating cosine similarity between texts in our embedding store and given examples. This function leverages Teradata's Massive Parallel Processing (MPP) capabilities, enabling high-performance and scalable computation.\n",
    "\n",
    "The `TD_VectorDistance` function computes the cosine similarity between the query embedding (representing the given example) and the embeddings stored in our embedding store. By comparing the angles between vectors in the multi-dimensional space, the function identifies the most semantically similar emails to the given example.\n",
    "\n",
    "In this specific case, we aim to collect the most semantically similar emails by the given example. This allows us to efficiently identify relevant content and extract valuable insights from our email dataset.\n",
    "\n",
    "By utilizing Teradata's powerful processing capabilities and in-database functions like `TD_VectorDistance`, we can perform advanced semantic search operations with unparalleled performance and scalability. This enables us to effectively analyze large volumes of text data and extract meaningful information, facilitating data-driven decision-making and enhancing business outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd41dc-d906-4c59-ba9a-6283b90c63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_embeddings_store_tgt = tdf_embeddings_store[tdf_embeddings_store.id == 3]\n",
    "tdf_embeddings_store_ref = tdf_embeddings_store[tdf_embeddings_store.id != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce4063-339b-47c4-8e19-a2379aeb67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.DataFrame.from_query(f\"\"\"\n",
    "\n",
    "SELECT \n",
    "    dt.target_id, \n",
    "    dt.reference_id,\n",
    "    e_tgt.txt as target_txt,\n",
    "    e_ref.txt as reference_txt,\n",
    "    e_tgt.txt_jp as target_txt_jp,\n",
    "    e_ref.txt_jp as reference_txt_jp,\n",
    "    (1.0 - dt.distance) as similiarity \n",
    "FROM\n",
    "    TD_VECTORDISTANCE (\n",
    "        ON (%s) AS TargetTable\n",
    "        ON (%s) AS ReferenceTable DIMENSION\n",
    "        USING\n",
    "            TargetIDColumn('id')\n",
    "            TargetFeatureColumns('[emb_0:emb_767]')\n",
    "            RefIDColumn('id')\n",
    "            RefFeatureColumns('[emb_0:emb_767]')\n",
    "            DistanceMeasure('cosine')\n",
    "            topk(3)\n",
    "    ) AS dt\n",
    "JOIN emails.emails_jp e_tgt on e_tgt.id = dt.target_id\n",
    "JOIN emails.emails_jp e_ref on e_ref.id = dt.reference_id;\n",
    "\"\"\"%(tdf_embeddings_store_tgt.show_query(), tdf_embeddings_store_ref.show_query()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6894637c-9adb-4ab6-9541-c99bc4370096",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.remove_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eda56c-e5b2-47e9-9a17-3ff923a0a18a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
