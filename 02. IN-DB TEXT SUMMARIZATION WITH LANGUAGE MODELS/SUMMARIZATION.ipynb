{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10d508d-9777-4dff-aaa7-49cd256fa85b",
   "metadata": {},
   "source": [
    "# Running Generative Models Inside Teradata DB\n",
    "\n",
    "This demo demonstrates how to run generative models inside the Teradata database. To achieve this, models must first be converted to the ONNX format.\n",
    "\n",
    "## Introduction to Generative Language Models\n",
    "\n",
    "Generative language models are a type of artificial intelligence designed to generate new content based on the data they have been trained on. These models can produce human-like text, translate languages, answer questions, and even create summaries of long documents. They are trained on vast amounts of text data and learn to predict the next word in a sequence, which allows them to generate coherent and contextually relevant sentences.\n",
    "\n",
    "### T5 Model for Summarization\n",
    "\n",
    "In this demo, we will use the T5 (Text-to-Text Transfer Transformer) model for summarization. The T5 model, developed by Google, is a versatile generative language model that converts all NLP tasks into a text-to-text format. This means that both the input and output are always text strings, making the model highly flexible and capable of handling a wide range of tasks, from translation to summarization to question answering.\n",
    "\n",
    "### Encoder-Decoder Models\n",
    "\n",
    "The T5 model is an example of an encoder-decoder model. Encoder-decoder models consist of two main components:\n",
    "\n",
    "1. **Encoder**: The encoder reads the input text and transforms it into a fixed-size context vector. This vector captures the essential information and context from the input text.\n",
    "2. **Decoder**: The decoder takes the context vector produced by the encoder and generates the output text. It uses the information in the context vector to produce a coherent and contextually appropriate response.\n",
    "\n",
    "This architecture allows encoder-decoder models to handle complex tasks that require understanding and generating text, such as summarization and translation. The T5 model's ability to treat every NLP task as a text-to-text problem simplifies the process of applying the model to different tasks, making it a powerful tool for various applications.\n",
    "\n",
    "One advanced technique often used with encoder-decoder models is **Beam Search**. Beam Search is a decoding algorithm that improves the quality of generated sequences. Instead of greedily choosing the most probable token at each step, Beam Search keeps track of multiple candidate sequences (called beams) at each step. It expands these beams by considering the top tokens for each candidate sequence and retains only the most promising ones based on their cumulative probabilities. By exploring multiple potential sequences simultaneously, Beam Search increases the likelihood of finding a high-quality output.\n",
    "\n",
    "In this demo, we will walk through the steps required to convert the T5 model to the ONNX format and run it within the Teradata database to perform text summarization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4b965-46db-4588-871b-f3e610527884",
   "metadata": {},
   "source": [
    "## Demo Flow Overview\n",
    "\n",
    "This demo showcases the process of running generative models inside the Teradata database. The entire workflow is divided into three main steps:\n",
    "\n",
    "1. **Model Import and Conversion to ONNX**:\n",
    "   The first step involves importing a pre-trained T5 model from the Hugging Face library and converting it to the ONNX (Open Neural Network Exchange) format. This step ensures that our model is ready for efficient deployment and execution within the Teradata environment.\n",
    "\n",
    "2. **Deployment of the Model and Tokenizer to Teradata**:\n",
    "   In the second step, we establish a connection to the Teradata database using the `teradataml` Python library. This library handles all aspects of connectivity and provides a user-friendly API similar to PySpark or Pandas DataFrame. We deploy two artifacts to the database: the ONNX model itself and the `tokenizer.json` file. These artifacts are deployed using the `save_byom` function, which abstracts the underlying complexity of the deployment process.\n",
    "\n",
    "3. **In-Database Inference for Text Summarization**:\n",
    "   The final step involves performing in-database inference to generate text summarizations. Using the deployed model and tokenizer, we process the input texts directly within the database. This approach leverages Teradata's massive parallel processing capabilities, providing performance and scalability advantages. Additionally, keeping the data within the database enhances security and reduces data transfer overhead.\n",
    "\n",
    "By following these steps, we demonstrate how to effectively run generative models within the Teradata database, highlighting the benefits of keeping data and computation close together for enhanced performance and security.\n",
    "\n",
    "![alt text](img/summarization_workflow.jpg \"Teradata in-database LLMs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c028a-cfb3-4a68-900b-c974cb79b5ae",
   "metadata": {},
   "source": [
    "## Step 1. Model Import and Conversion to ONNX\n",
    "\n",
    "\n",
    "To run the T5 model within the Teradata database, we first need to convert it to the ONNX (Open Neural Network Exchange) format. ONNX models are typically more performant in inference compared to models executed natively, thanks to optimizations that streamline their execution across different platforms and hardware. ONNX is an open-source format for AI models, which allows models to be transferred between different frameworks and run on various hardware platforms.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   We start by importing the T5 model from the Hugging Face library using the utility built into the `onnxruntime` package. Hugging Face provides a wide range of pre-trained models, making it easy to access and use state-of-the-art NLP models. Using the this utility, we convert the T5 model from its native PyTorch format to the ONNX format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811eee24-4409-4850-af3f-7ba04e703b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "import onnx\n",
    "\n",
    "from onnxruntime.tools.onnx_model_utils import *\n",
    "\n",
    "import transformers\n",
    "\n",
    "import warnings\n",
    "\n",
    "import teradataml as tdml\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861c1e4-468b-4702-92e0-01c539f22e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the execution may ends with error about IR version at the tests but this is OK. \n",
    "# This error cause by Linux specifics of this particular DemoVM\n",
    "! python3 -m onnxruntime.transformers.convert_generation --total_runs 0 --disable_perf_test --disable_parity -m JulesBelveze/t5-small-headline-generator --model_type t5 --output t5-small-headline-generator/t5-small-headline-generator.onnx --no_repeat_ngram_size 2  --custom_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9e71d-a2de-415e-ac17-3cb794a944ce",
   "metadata": {},
   "source": [
    "   After conversion, we adjust the opset version in the ONNX file to match the compatibility requirements of the Teradata environment. Opset versions define the operations that are available for use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a87f008-5db8-4fe2-84af-eb9b3cdb99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = onnx.OperatorSetIdProto()\n",
    "op.version = 12\n",
    "\n",
    "model = onnx.load('./t5-small-headline-generator/t5-small-headline-generator.onnx')\n",
    "\n",
    "model_ir8 = onnx.helper.make_model(model.graph, ir_version = 8, opset_imports = [op])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf6919-90aa-49d7-8d74-c651cf1fbc83",
   "metadata": {},
   "source": [
    "We need to fix the dynamic dimensions on the input and output to ensure the model operates correctly within the database. This involves setting specific dimensions that the model will use during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37644549-70bc-4b37-8ee7-63d10f7e7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"sequence_length\", 512)\n",
    "\n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"num_return_sequences\", 1)\n",
    "rt.tools.onnx_model_utils.make_dim_param_fixed(model_ir8.graph, \"max_length\", 100)\n",
    "\n",
    "\n",
    "onnx.save(model_ir8, './t5-small-headline-generator/t5-small-headline-generator_fixed.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bc72f0-4588-4227-8e4d-a7d2a23b446a",
   "metadata": {},
   "source": [
    "We test the ONNX model to ensure that it produces reasinable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a13746-ff1e-4620-96e0-61b8dc7b4dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.T5TokenizerFast.from_pretrained(\"JulesBelveze/t5-small-headline-generator\")\n",
    "predef_sess = rt.InferenceSession(\"./t5-small-headline-generator/t5-small-headline-generator_fixed.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e78c2b-1c6f-4214-a919-b64884effb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer(\"\"\"\n",
    "During my trip to Dubai on February 10th, I had a meeting with the CEO of Green Energy Solutions. Our collaboration involves a budget of $90,000 for renewable energy projects, targeting a 30% increase in clean energy adoption.\n",
    "\"\"\", max_length = 512, padding='max_length')\n",
    "\n",
    "\n",
    "encoder_result = predef_sess.run(\n",
    "    None, \n",
    "    {\"input_ids\": [enc.input_ids], \n",
    "     \"attention_mask\": [enc.attention_mask],\n",
    "     \"max_length\": [100], \n",
    "     \"min_length\": [10], \n",
    "     \"repetition_penalty\": [2],\n",
    "     'num_beams' : [4], \n",
    "     'num_return_sequences': [1], \n",
    "     'length_penalty': [0]})\n",
    "\n",
    "\n",
    "tokenizer.decode(encoder_result[0][0][0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3bacd5-29d5-4ce5-991d-ee1c530383f2",
   "metadata": {},
   "source": [
    "And finally we save the tokenizer definition to a JSON file. This file will used during the in-database processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30167fe0-d5eb-4bc7-81fa-7594b715bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"t5-small-headline-generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7252cfa-9f34-492e-b1ca-3725c917852f",
   "metadata": {},
   "source": [
    "## Part 2. Model Deployment to Database to be Used with BYOM\n",
    "\n",
    "In this section, we demonstrate how to deploy the model to the Teradata database using the BYOM (Bring Your Own Model) capability. We use the `teradataml` Python library to manage the connectivity and provide a convenient Python API that is similar to PySpark or pandas DataFrame.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717690ba-fb02-4cd2-9092-66ed863c72c3",
   "metadata": {},
   "source": [
    "### Opening Connection to Teradata\n",
    "\n",
    "We start by setting up a connection to the Teradata database. The `teradataml` library handles all the intricacies of database connectivity, allowing us to interact with Teradata in a manner similar to working with data in pandas DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee27d0-2fae-4f1f-93bf-aaa4a8b0b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.create_context(host = '192.168.178.232', username='<YOUR DATABASE USERNAME>', password = '<YOUR DATABASE PASSWORD>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c21f5d-7327-4d6e-b5fd-8bbb975f8057",
   "metadata": {},
   "source": [
    "### Deploying the Model and Tokenizer\n",
    "\n",
    "After establishing the connection, we deploy two key artifacts to the database:\n",
    "1. The model itself, converted to ONNX format.\n",
    "2. The `tokenizer.json` file, which will be used for in-database tokenization.\n",
    "\n",
    "Both artifacts are deployed using the `save_byom` function, which abstracts the underlying complexity and makes the deployment process straightforward. Internally, this function performs an insert operation into the database.\n",
    "\n",
    "By using the `save_byom` function, we ensure that our model and tokenizer are readily available within the Teradata database for subsequent summarization operations. This integration minimizes data movement and optimizes performance by keeping all operations within the database environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ef807-22f7-4371-b3b8-0af071e816b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT IF TABLE EXISTS\n",
    "#tdml.db_drop_table('summarization_models')\n",
    "#THIS OPERATION MAY TAKE A WHILE\n",
    "tdml.save_byom('t5-small-headline-generator',\n",
    "              './t5-small-headline-generator/t5-small-headline-generator_fixed.onnx',\n",
    "              'summarization_models')\n",
    "\n",
    "#UNCOMMENT IF TABLE EXISTS\n",
    "#tdml.db_drop_table('summarization_tokenizers')\n",
    "tdml.save_byom('t5-small-headline-generator',\n",
    "              './t5-small-headline-generator/tokenizer.json',\n",
    "              'summarization_tokenizers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ff39e-5f46-4396-bf89-b0a71042085d",
   "metadata": {},
   "source": [
    "## Part 3. In-Database Inference for Text Summarization\n",
    "\n",
    "Running a summarization model directly inside the Teradata database offers numerous benefits, including enhanced security, reduced data transfer overhead, and the utilization of Teradata's robust computational capabilities. This setup enables efficient, scalable text processing workflows, which is particularly advantageous for summarizing large volumes of text such as email communications.\n",
    "\n",
    "In this demonstration, we'll use a small table of made-up emails as an example to illustrate how the summarization model works in-database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce91af9-d0bf-4053-a1e3-6f8a0bbeac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.DataFrame(tdml.in_schema('emails', 'emails')).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426fe60-f3a3-4273-9003-d375149eddf4",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The first sub-step involves creating a view with a tokenization function. This function converts the original text into binary vectors of tokens, which are the model-readable formats of the texts. Tokenization is a crucial process where text strings are broken down into smaller pieces or tokens. In Teradata, you can utilize any tokenizer from the Hugging Face models, which allows flexibility depending on the specific model or language nuances.\n",
    "\n",
    "During tokenization, not only are token IDs generated, but an attention mask is also produced. The attention mask is an array of 1s and 0s indicating which tokens should be attended to, and which should be ignored by the model. This is essential for models to handle variable length inputs effectively and is particularly useful for padding sequences to a uniform length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4555e-ea80-49bd-953c-6f31befde002",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.execute_sql(\"\"\"\n",
    "\n",
    "replace view v_emails_tokenized as (\n",
    "    select\n",
    "        id,\n",
    "        txt,\n",
    "        IDS as input_ids,\n",
    "        attention_mask,\n",
    "        cast(50 as BIGINT) max_length_0,\n",
    "        cast(10 as BIGINT) min_length_0,\n",
    "        cast(4 as BIGINT) num_beams_0,\n",
    "        cast(1 as BIGINT) num_return_sequences_0,\n",
    "        cast(1 as FLOAT) repetition_penalty_0,\n",
    "        cast(2 as FLOAT) length_penalty_0\n",
    "    from ivsm.tokenizer_encode(\n",
    "        on emails.emails\n",
    "        on (select model as tokenizer from summarization_tokenizers) DIMENSION\n",
    "        USING\n",
    "            ColumnsToPreserve('id', 'txt')\n",
    "            OutputFields('IDS', 'ATTENTION_MASK')\n",
    "            MaxLength(512)\n",
    "            PadToMaxLength('True')\n",
    "            TokenDataType('INT32')\n",
    "            Debug('True')\n",
    "    ) a\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943fac75-b5c2-401b-917a-30e02be19f33",
   "metadata": {},
   "source": [
    "### Model Application\n",
    "\n",
    "The second sub-step is the actual application of the model in its ONNX format to the token vectors. When applying the model, parameters such as the number of beams for Beam Search, along with repetition and length penalties, are provided. These parameters are critical for controlling the generation process:\n",
    "   - **Number of Beams**: This parameter for Beam Search controls how many different paths or 'beams' are considered during the decoding phase, allowing for more thorough exploration of possible translations or summaries\n",
    "   - **Repetition Penalty**: This discourages the model from repeating the same line or phrase, enhancing the diversity and naturalness of the generated text\n",
    "   - **Length Penalty**: This adjusts the model's preference for longer or shorter sentences, helping to ensure the output matches desired verbosity or succinctness\n",
    "\n",
    "Running the model in-database capitalizes on Teradata's inbuilt efficiencies, ensuring that these computations are performed swiftly and at scale. The output from this step is again in the form of binary vectors representing the tokens of the summarized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986e161-1841-48d2-8e72-8e98d507a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.execute_sql(\"\"\"\n",
    "replace view v_emails_encoded\n",
    "as (\n",
    "    select \n",
    "            *\n",
    "    from ivsm.IVSM_score(\n",
    "            on v_emails_tokenized  -- table with data to be scored\n",
    "            on summarization_models dimension\n",
    "            using\n",
    "                ColumnsToPreserve('id', 'txt') -- columns to be copied from input table\n",
    "                ModelType('ONNX') -- model format\n",
    "                BinaryInputFields('input_ids', 'attention_mask') -- enables binary input vectors\n",
    "                BinaryOutputFields('sequences') -- define which output tensors to be outputed in binary format\n",
    "                Caching('interquery') -- trun on model caching within the query\n",
    "        ) a )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e55078-6b68-4ef4-acbf-882f77268a65",
   "metadata": {},
   "source": [
    "### Detokenization\n",
    "\n",
    "The final sub-step is detokenization, where the binary vector outputs from the model are converted back into human-readable text. This process is the inverse of tokenization and is essential for transforming the model's output into a format that is easily understandable by humans. Detokenization reaffirms the seamless integration of sophisticated NLP models within database operations, bridging the gap between advanced AI computations and practical business applications.\n",
    "\n",
    "By executing these steps within the Teradata database, we harness the full power of in-database analytics to perform complex text summarization tasks directly where the data resides, reducing latency and enhancing overall data management efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc49687-87b6-45c9-87ae-93174f94f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT IF TABLE EXISTS\n",
    "#tdml.db_drop_table(\"emails_processed\")\n",
    "tdml.execute_sql(\"\"\"\n",
    "\n",
    "create table emails_processed as \n",
    "(\n",
    "    select\n",
    "        *\n",
    "    from ivsm.tokenizer_decode(\n",
    "        on (select id, txt, sequences as vector from v_emails_encoded)\n",
    "        on (select model as tokenizer from summarization_tokenizers) DIMENSION\n",
    "        USING\n",
    "            ColumnsToPreserve('id', 'txt')\n",
    "            TokenDataType('INT32')\n",
    "            SkipSpecialTokens('False')\n",
    "    ) a\n",
    ") with data\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a2173-007b-40c0-951d-47866424f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdml.DataFrame(\"emails_processed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
